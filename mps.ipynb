{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QR Compression\n",
    "\n",
    "Here's an example of how a conversion of a big tensor to an MPS form could go: we iteratively sweep doing QR decompositions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_mps_qr(w):\n",
    "\n",
    "    nspins = int(np.log2(np.size(w)))\n",
    "    print(f\"{nspins=}\")\n",
    "\n",
    "    mps_tensors = []\n",
    "    # first tensor \n",
    "    w = np.reshape(w, (2,2**(nspins-1)))\n",
    "    q,r = np.linalg.qr(w)\n",
    "    mps_tensors.append(q)\n",
    "    virtual_dim = np.shape(q)[1]\n",
    "    print(f\"{virtual_dim=}\")\n",
    "\n",
    "    for nn in range(2,nspins):\n",
    "        print(f\"Factorizing {nn}th tensor\")\n",
    "        #print(f\"{np.shape(r)=} \")\n",
    "        r = np.reshape(r,(virtual_dim*2,2**(nspins-nn)))\n",
    "        #print(f\"{np.shape(r)=}\")\n",
    "        q, r = np.linalg.qr(r)\n",
    "        mps_tensors.append(np.reshape(q,(virtual_dim,2,np.shape(q)[1])))\n",
    "        virtual_dim = np.shape(q)[1]\n",
    "        print(f\"{virtual_dim=}\")\n",
    "\n",
    "    mps_tensors.append(np.reshape(r, (virtual_dim, 2)))\n",
    "\n",
    "\n",
    "    return mps_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if this works: build a state for say 10 qubits, and construct the MPS tensors from it.\n",
    "\n",
    "We can also check what the sizes of the matrices involved are, \n",
    "in principle the maximum bond dimension allowed is 2^(N/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nspins=10\n",
      "virtual_dim=2\n",
      "Factorizing 2th tensor\n",
      "virtual_dim=4\n",
      "Factorizing 3th tensor\n",
      "virtual_dim=8\n",
      "Factorizing 4th tensor\n",
      "virtual_dim=16\n",
      "Factorizing 5th tensor\n",
      "virtual_dim=32\n",
      "Factorizing 6th tensor\n",
      "virtual_dim=16\n",
      "Factorizing 7th tensor\n",
      "virtual_dim=8\n",
      "Factorizing 8th tensor\n",
      "virtual_dim=4\n",
      "Factorizing 9th tensor\n",
      "virtual_dim=2\n",
      "0 - (2, 2)\n",
      "1 - (2, 2, 4)\n",
      "2 - (4, 2, 8)\n",
      "3 - (8, 2, 16)\n",
      "4 - (16, 2, 32)\n",
      "5 - (32, 2, 16)\n",
      "6 - (16, 2, 8)\n",
      "7 - (8, 2, 4)\n",
      "8 - (4, 2, 2)\n",
      "9 - (2, 2)\n"
     ]
    }
   ],
   "source": [
    "w = (np.random.rand(1024) + 1j*np.random.rand(1024) )\n",
    "w = w/np.linalg.norm(w)\n",
    "\n",
    "mpsten = tensor_to_mps_qr(w)\n",
    "\n",
    "len(mpsten)\n",
    "\n",
    "for ii,ten in enumerate(mpsten):\n",
    "    print(f\"{ii} - {np.shape(ten)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to reconstruct the original tensor from the MPS tensors, to make sure that we are doing things right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to reconstruct \n",
    "\n",
    "def wten_from_mps(mps_list):\n",
    "\n",
    "    # First tensor\n",
    "    w_rec = mps_list[0]\n",
    "\n",
    "    for jj in range(1,len(mps_list)-1):\n",
    "        print(f\"{jj} - {np.shape(w_rec)} * {np.shape(mps_list[jj])}\")\n",
    "        w_rec = np.einsum(\"ia,ajb->ijb\", w_rec, mps_list[jj])\n",
    "        w_rec = np.reshape(w_rec, (np.shape(w_rec)[0] * np.shape(w_rec)[1], np.shape(w_rec)[2]))\n",
    "\n",
    "    # Final tensor \n",
    "    w_rec = np.einsum(\"ia,aj->ij\", w_rec, mps_list[-1])\n",
    "    w_rec = np.reshape(w_rec, (np.shape(w_rec)[0] * np.shape(w_rec)[1]))\n",
    "\n",
    "\n",
    "    return w_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - (2, 2) * (2, 2, 4)\n",
      "2 - (4, 4) * (4, 2, 8)\n",
      "3 - (8, 8) * (8, 2, 16)\n",
      "4 - (16, 16) * (16, 2, 32)\n",
      "5 - (32, 32) * (32, 2, 16)\n",
      "6 - (64, 16) * (16, 2, 8)\n",
      "7 - (128, 8) * (8, 2, 4)\n",
      "8 - (256, 4) * (4, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_reconstructed = wten_from_mps(mpsten)\n",
    "\n",
    "np.allclose(w, w_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting matrices have a specific property: due to the way we did our sweep, \n",
    "\n",
    "the MPS will be in *left* canonical form. Let's check it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00-1.05480848e-17j, -5.55111512e-17+2.22044605e-16j],\n",
       "       [-5.55111512e-17-2.22044605e-16j,  1.00000000e+00-4.92560725e-18j]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum(\"pv,pw->vw\", mpsten[0],mpsten[0].conj())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00+1.67544707e-17j,  5.55111512e-17+6.24500451e-17j,\n",
       "         0.00000000e+00-9.97465999e-18j, -4.85722573e-17+0.00000000e+00j],\n",
       "       [ 5.55111512e-17-6.24500451e-17j,  1.00000000e+00+8.17970144e-18j,\n",
       "         0.00000000e+00+2.77555756e-17j, -1.38777878e-17+1.38777878e-16j],\n",
       "       [ 0.00000000e+00+3.42607887e-17j,  0.00000000e+00-2.08166817e-17j,\n",
       "         1.00000000e+00-3.08544155e-18j, -1.38777878e-17-2.77555756e-17j],\n",
       "       [-4.85722573e-17+0.00000000e+00j, -1.38777878e-17-1.11022302e-16j,\n",
       "        -1.38777878e-17+2.77555756e-17j,  1.00000000e+00-1.60777876e-17j]])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.einsum(\"lpr,lpk->rk\", mpsten[1],mpsten[1].conj())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lld = np.einsum(\"lpr,lpk->rk\", mpsten[3],mpsten[3].conj())\n",
    "print(np.shape(lld))\n",
    "np.allclose(lld, np.eye(np.shape(lld)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1:  Factorization using SVD \n",
    "\n",
    "We can do something analogous using SVD. While QR is faster, SVD allows us to \n",
    "estimate the entanglement of the state, and will allow us to truncate in a controlled way.\n",
    "\n",
    "Write a function analogous to `tensor_to_mps_qr()` which instead uses SVD to factorize the state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_mps_svd(w):\n",
    "\n",
    "    nspins = int(np.log2(np.size(w)))\n",
    "\n",
    "    print(f\"{nspins=}\")\n",
    "\n",
    "    mps_tensors = []\n",
    "    # first tensor \n",
    "    w = np.reshape(w, (2,2**(nspins-1)))\n",
    "    u,s,vd = np.linalg.svd(w,full_matrices=False)\n",
    "    mps_tensors.append(u)\n",
    "    virtual_dim = np.shape(u)[1]\n",
    "    print(f\"{virtual_dim=}\")\n",
    "    s_vd = np.einsum(\"ab,bj->aj\",np.diag(s),vd)\n",
    "\n",
    "    for nn in range(2,nspins):\n",
    "        print(f\"Factorizing {nn}th tensor\")\n",
    "        #print(f\"{np.shape(r)=} \")\n",
    "        s_vd = np.reshape(s_vd,(virtual_dim*2,2**(nspins-nn)))\n",
    "        #print(f\"{np.shape(r)=}\")\n",
    "        u, s, vd = np.linalg.svd(s_vd,full_matrices=False)\n",
    "        mps_tensors.append(np.reshape(u,(virtual_dim,2,np.shape(u)[1])))\n",
    "        virtual_dim = np.shape(u)[1]\n",
    "        print(f\"{virtual_dim=}\")\n",
    "        s_vd = np.einsum(\"ab,bj->aj\",np.diag(s),vd)\n",
    "\n",
    "\n",
    "    mps_tensors.append(np.reshape(s_vd, (virtual_dim, 2)))\n",
    "\n",
    "\n",
    "    return mps_tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test whether this works "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nspins=10\n",
      "virtual_dim=2\n",
      "Factorizing 2th tensor\n",
      "virtual_dim=4\n",
      "Factorizing 3th tensor\n",
      "virtual_dim=8\n",
      "Factorizing 4th tensor\n",
      "virtual_dim=16\n",
      "Factorizing 5th tensor\n",
      "virtual_dim=32\n",
      "Factorizing 6th tensor\n",
      "virtual_dim=16\n",
      "Factorizing 7th tensor\n",
      "virtual_dim=8\n",
      "Factorizing 8th tensor\n",
      "virtual_dim=4\n",
      "Factorizing 9th tensor\n",
      "virtual_dim=2\n",
      "0 - (2, 2)\n",
      "1 - (2, 2, 4)\n",
      "2 - (4, 2, 8)\n",
      "3 - (8, 2, 16)\n",
      "4 - (16, 2, 32)\n",
      "5 - (32, 2, 16)\n",
      "6 - (16, 2, 8)\n",
      "7 - (8, 2, 4)\n",
      "8 - (4, 2, 2)\n",
      "9 - (2, 2)\n",
      "1 - (2, 2) * (2, 2, 4)\n",
      "2 - (4, 4) * (4, 2, 8)\n",
      "3 - (8, 8) * (8, 2, 16)\n",
      "4 - (16, 16) * (16, 2, 32)\n",
      "5 - (32, 32) * (32, 2, 16)\n",
      "6 - (64, 16) * (16, 2, 8)\n",
      "7 - (128, 8) * (8, 2, 4)\n",
      "8 - (256, 4) * (4, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = (np.random.rand(1024) + 1j*np.random.rand(1024) )\n",
    "w = w/np.linalg.norm(w)\n",
    "\n",
    "mpsten = tensor_to_mps_svd(w)\n",
    "\n",
    "len(mpsten)\n",
    "\n",
    "for ii,ten in enumerate(mpsten):\n",
    "    print(f\"{ii} - {np.shape(ten)}\")\n",
    "\n",
    "w_reconstructed = wten_from_mps(mpsten)\n",
    "\n",
    "np.allclose(w, w_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99498793, 0.09949879])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = [1,0.1,0.01,0.00001]\n",
    "v1 /= np.linalg.norm(v1)\n",
    "\n",
    "v1[v1 > 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cut=500\n",
      "Last SV = 7.96016412744189e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[-0.03134922, -0.04192444,  0.01019647, ..., -0.00565838,\n",
       "          0.02213023,  0.0366036 ],\n",
       "        [-0.03176487, -0.00177647,  0.00019864, ...,  0.01507021,\n",
       "         -0.0428183 ,  0.01533865],\n",
       "        [-0.03141648, -0.01652561, -0.03680401, ..., -0.02713592,\n",
       "         -0.0479172 , -0.00041648],\n",
       "        ...,\n",
       "        [-0.03115906, -0.01308556,  0.0301592 , ..., -0.01950727,\n",
       "         -0.0629922 , -0.00266309],\n",
       "        [-0.03157493, -0.05849522,  0.01041717, ..., -0.00277386,\n",
       "         -0.03694582, -0.02716371],\n",
       "        [-0.03129119, -0.02455811,  0.02962391, ..., -0.02021741,\n",
       "          0.0418008 ,  0.06416723]]),\n",
       " array([500.07064274,  18.1534261 ,  18.03892091,  17.96823521,\n",
       "         17.96104582,  17.81855864,  17.75486459,  17.6872707 ,\n",
       "         17.6648036 ,  17.59961177,  17.56122444,  17.44835445,\n",
       "         17.39318307,  17.33400271,  17.29847596,  17.25086915,\n",
       "         17.21872288,  17.1533544 ,  17.10160437,  17.07694838,\n",
       "         17.04105226,  16.98582467,  16.96670259,  16.93735871,\n",
       "         16.89051742,  16.87456869,  16.84979588,  16.80083702,\n",
       "         16.7674471 ,  16.70189458,  16.66365397,  16.6250518 ,\n",
       "         16.57261991,  16.55054542,  16.53493648,  16.49035715,\n",
       "         16.46510794,  16.43497734,  16.40521102,  16.37618992,\n",
       "         16.33683069,  16.28872419,  16.26830863,  16.26250554,\n",
       "         16.21463834,  16.21250486,  16.17654877,  16.1293504 ,\n",
       "         16.05845845,  16.03794404,  16.00288625,  15.98009878,\n",
       "         15.95754999,  15.91936056,  15.90068932,  15.87021102,\n",
       "         15.84141022,  15.80366318,  15.79135808,  15.75574912,\n",
       "         15.7364617 ,  15.71268153,  15.69171939,  15.66202971,\n",
       "         15.64234826,  15.59208224,  15.57970316,  15.55914251,\n",
       "         15.50482936,  15.47788912,  15.45325455,  15.42416328,\n",
       "         15.40443429,  15.36675182,  15.36102332,  15.32419385,\n",
       "         15.28527889,  15.27333847,  15.23137696,  15.20964219,\n",
       "         15.1786825 ,  15.15164386,  15.12097633,  15.09842395,\n",
       "         15.06940328,  15.04763984,  15.02176481,  15.00607328,\n",
       "         14.94550232,  14.94093433,  14.91070274,  14.90036606,\n",
       "         14.86327277,  14.84540851,  14.83633979,  14.81956858,\n",
       "         14.77876527,  14.72291932,  14.7074288 ,  14.69555374,\n",
       "         14.65879657,  14.64620514,  14.62450351,  14.60458316,\n",
       "         14.55686319,  14.55420092,  14.52004116,  14.51114469,\n",
       "         14.49040117,  14.46551684,  14.42224113,  14.40131333,\n",
       "         14.37180662,  14.35182201,  14.34178333,  14.31127407,\n",
       "         14.29238049,  14.27519504,  14.24215117,  14.20782005,\n",
       "         14.20021572,  14.16563964,  14.15038453,  14.11804199,\n",
       "         14.10335333,  14.07015563,  14.04368317,  14.02855838,\n",
       "         14.01040062,  13.99570015,  13.98670335,  13.96208344,\n",
       "         13.92702507,  13.91734174,  13.91136498,  13.89582423,\n",
       "         13.86448892,  13.83338544,  13.8210408 ,  13.78954518,\n",
       "         13.77684917,  13.74403657,  13.73721425,  13.70462539,\n",
       "         13.68467746,  13.67151845,  13.65192229,  13.62297617,\n",
       "         13.6085526 ,  13.60351798,  13.55940947,  13.54151381,\n",
       "         13.52443329,  13.50501117,  13.50331438,  13.47352964,\n",
       "         13.44554519,  13.41664307,  13.40640922,  13.38431845,\n",
       "         13.36729082,  13.33762326,  13.3331968 ,  13.29866866,\n",
       "         13.26970165,  13.24201234,  13.23787141,  13.2087991 ,\n",
       "         13.19058006,  13.17403042,  13.16489047,  13.14616845,\n",
       "         13.10586513,  13.09829986,  13.08613574,  13.07177443,\n",
       "         13.05360548,  13.027164  ,  12.98649695,  12.94860738,\n",
       "         12.93143429,  12.90208529,  12.89144293,  12.86509182,\n",
       "         12.85438729,  12.82847654,  12.82161856,  12.81689245,\n",
       "         12.78024181,  12.77123851,  12.73642153,  12.71972356,\n",
       "         12.66597007,  12.65690518,  12.65151559,  12.63846653,\n",
       "         12.61349594,  12.60056293,  12.56814047,  12.54308687,\n",
       "         12.53240922,  12.53063418,  12.48621922,  12.4695286 ,\n",
       "         12.46041294,  12.43503864,  12.41118277,  12.38923292,\n",
       "         12.37876041,  12.36833176,  12.33731844,  12.32309507,\n",
       "         12.27830333,  12.26646922,  12.25284495,  12.23932434,\n",
       "         12.22581592,  12.22272188,  12.20566473,  12.17582264,\n",
       "         12.15517342,  12.14811678,  12.12055783,  12.10096718,\n",
       "         12.0782752 ,  12.0559738 ,  12.03031476,  12.01388667,\n",
       "         11.99989428,  11.99678183,  11.94649038,  11.92406772,\n",
       "         11.908901  ,  11.89124582,  11.87024364,  11.86325416,\n",
       "         11.83795896,  11.82713761,  11.78938881,  11.76793861,\n",
       "         11.75268227,  11.73714217,  11.7253853 ,  11.70286333,\n",
       "         11.68627204,  11.68338434,  11.64684835,  11.63825664,\n",
       "         11.62295027,  11.61254091,  11.59335662,  11.57033363,\n",
       "         11.53301955,  11.51148231,  11.5002871 ,  11.49457053,\n",
       "         11.47941718,  11.47473386,  11.44475695,  11.42193315,\n",
       "         11.40199721,  11.38266879,  11.36955606,  11.36198136,\n",
       "         11.32066156,  11.2994376 ,  11.27718412,  11.26475432,\n",
       "         11.25953056,  11.2456226 ,  11.23538045,  11.18343459,\n",
       "         11.17882964,  11.17339619,  11.12384684,  11.1153785 ,\n",
       "         11.09982206,  11.0894629 ,  11.06543957,  11.04960581,\n",
       "         11.01546856,  10.99772281,  10.98558634,  10.96988963,\n",
       "         10.92725647,  10.92234566,  10.91611695,  10.89212113,\n",
       "         10.86539302,  10.8576877 ,  10.84741952,  10.82552909,\n",
       "         10.77758147,  10.76968304,  10.7506378 ,  10.74687167,\n",
       "         10.72004955,  10.71082638,  10.70595255,  10.6945513 ,\n",
       "         10.69033138,  10.63384603,  10.62758191,  10.61018631,\n",
       "         10.60575621,  10.58840057,  10.57008681,  10.55511012,\n",
       "         10.52887967,  10.52443167,  10.506453  ,  10.49664812,\n",
       "         10.47293275,  10.4459681 ,  10.41542966,  10.39144283,\n",
       "         10.37122671,  10.36027727,  10.34950328,  10.33546307,\n",
       "         10.31123819,  10.29776328,  10.28422744,  10.26006387,\n",
       "         10.24608112,  10.21846176,  10.2081892 ,  10.1847382 ,\n",
       "         10.17491426,  10.17063044,  10.14432895,  10.13824386,\n",
       "         10.10551249,  10.09410805,  10.07308598,  10.06730003,\n",
       "         10.04915252,  10.01708409,  10.01028121,  10.00564254,\n",
       "          9.97938212,   9.94948604,   9.9365122 ,   9.92997253,\n",
       "          9.91095994,   9.90043411,   9.87844618,   9.85435963,\n",
       "          9.83389767,   9.82750294,   9.80238628,   9.77676668,\n",
       "          9.75692232,   9.74640064,   9.73015852,   9.71561262,\n",
       "          9.69828659,   9.68050957,   9.66480782,   9.63344745,\n",
       "          9.62505478,   9.595393  ,   9.59247492,   9.5884229 ,\n",
       "          9.56221781,   9.53967058,   9.52181964,   9.51635387,\n",
       "          9.51293407,   9.47549861,   9.45646309,   9.43929418,\n",
       "          9.42826091,   9.40261276,   9.3879068 ,   9.36722819,\n",
       "          9.34465555,   9.3274137 ,   9.32205877,   9.31016188,\n",
       "          9.29659356,   9.28830564,   9.27589203,   9.23268226,\n",
       "          9.20194033,   9.19191265,   9.18051862,   9.16463134,\n",
       "          9.15439998,   9.12337671,   9.10760795,   9.09524515,\n",
       "          9.07933698,   9.05425361,   9.0428913 ,   9.03513352,\n",
       "          9.01339185,   9.00656534,   8.99405519,   8.98132434,\n",
       "          8.95831532,   8.93950284,   8.92680498,   8.91131561,\n",
       "          8.90888904,   8.87759469,   8.83680781,   8.83549017,\n",
       "          8.82145642,   8.8172709 ,   8.7834906 ,   8.77186097,\n",
       "          8.77134469,   8.74720077,   8.73556203,   8.71929513,\n",
       "          8.6855888 ,   8.67373133,   8.65817738,   8.64793726,\n",
       "          8.62205385,   8.61572678,   8.61130018,   8.59532692,\n",
       "          8.57468332,   8.55006309,   8.53296769,   8.53052044,\n",
       "          8.51785609,   8.48697591,   8.4767818 ,   8.46981493,\n",
       "          8.44714859,   8.42391229,   8.42033186,   8.38944282,\n",
       "          8.38223396,   8.37579235,   8.35702569,   8.35192682,\n",
       "          8.33694048,   8.30427389,   8.28409611,   8.27290695,\n",
       "          8.26040532,   8.2390944 ,   8.22460276,   8.20514549,\n",
       "          8.19327956,   8.18102372,   8.15845063,   8.15532733,\n",
       "          8.14827685,   8.13112264,   8.09295864,   8.06417385,\n",
       "          8.05576806,   8.03636064,   8.02719467,   8.00519584,\n",
       "          7.99796895,   7.97468607,   7.95773245,   7.94683971,\n",
       "          7.9286095 ,   7.92120278,   7.89673841,   7.89299975,\n",
       "          7.85378424,   7.84626745,   7.84045428,   7.8245627 ,\n",
       "          7.80291562,   7.79469298,   7.77224221,   7.75092912,\n",
       "          7.73580106,   7.7232551 ,   7.70644738,   7.67657856,\n",
       "          7.67060772,   7.65676278,   7.64626497,   7.63009597,\n",
       "          7.61146476,   7.58174917,   7.57086444,   7.56459398,\n",
       "          7.55030341,   7.53262623,   7.53210405,   7.51997887,\n",
       "          7.5001303 ,   7.49025919,   7.47539442,   7.46546626,\n",
       "          7.44778858,   7.44029767,   7.41285907,   7.40544966]),\n",
       " array([[-0.03187857, -0.03176814, -0.03261487, ..., -0.03210626,\n",
       "         -0.03242997, -0.03110291],\n",
       "        [ 0.0101965 , -0.040384  , -0.01254355, ...,  0.01855796,\n",
       "          0.00107353,  0.00579657],\n",
       "        [-0.09620272, -0.02350983, -0.00101525, ..., -0.02461673,\n",
       "         -0.02375812, -0.01867789],\n",
       "        ...,\n",
       "        [-0.04271634,  0.03145729,  0.01928311, ...,  0.01959891,\n",
       "          0.00674021,  0.02334841],\n",
       "        [-0.00366195, -0.0312242 , -0.00405707, ...,  0.02129648,\n",
       "          0.01634796, -0.01610796],\n",
       "        [-0.01246479, -0.00245373,  0.01211257, ..., -0.00262289,\n",
       "          0.03828934,  0.00850524]]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def svd_trunc(m,cutoff=1e-14,maxdim=500):\n",
    "    u, s, vd = np.linalg.svd(m,full_matrices=False)\n",
    "    snorm = s/np.linalg.norm(s)\n",
    "    cut = min(len(snorm[snorm > cutoff]), maxdim)\n",
    "    print(f\"{cut=}\")\n",
    "    print(f\"Last SV = {snorm[-1]}\")\n",
    "\n",
    "    return u[:,:cut], s[:cut], vd[:cut,:]\n",
    "\n",
    "svd_trunc(np.random.rand(1000,1000), cutoff=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_mps_svd_trunc(w):\n",
    "\n",
    "    nspins = int(np.log2(np.size(w)))\n",
    "\n",
    "    print(f\"{nspins=}\")\n",
    "\n",
    "    mps_tensors = []\n",
    "    # first tensor \n",
    "    w = np.reshape(w, (2,2**(nspins-1)))\n",
    "    u,s,vd = svd_trunc(w)\n",
    "    mps_tensors.append(u)\n",
    "    virtual_dim = np.shape(u)[1]\n",
    "    print(f\"{virtual_dim=}\")\n",
    "    s_vd = np.einsum(\"ab,bj->aj\",np.diag(s),vd)\n",
    "\n",
    "    for nn in range(2,nspins):\n",
    "        print(f\"Factorizing {nn}th tensor\")\n",
    "        #print(f\"{np.shape(r)=} \")\n",
    "        s_vd = np.reshape(s_vd,(virtual_dim*2,2**(nspins-nn)))\n",
    "        #print(f\"{np.shape(r)=}\")\n",
    "        u, s, vd = svd_trunc(s_vd)\n",
    "        mps_tensors.append(np.reshape(u,(virtual_dim,2,np.shape(u)[1])))\n",
    "        virtual_dim = np.shape(u)[1]\n",
    "        print(f\"{virtual_dim=}\")\n",
    "        s_vd = np.einsum(\"ab,bj->aj\",np.diag(s),vd)\n",
    "\n",
    "\n",
    "    mps_tensors.append(np.reshape(s_vd, (virtual_dim, 2)))\n",
    "\n",
    "\n",
    "    return mps_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nspins=10\n",
      "cut=2\n",
      "Last SV = 0.3572190917265257\n",
      "virtual_dim=2\n",
      "Factorizing 2th tensor\n",
      "cut=4\n",
      "Last SV = 0.23408990890403156\n",
      "virtual_dim=4\n",
      "Factorizing 3th tensor\n",
      "cut=8\n",
      "Last SV = 0.14124699061367757\n",
      "virtual_dim=8\n",
      "Factorizing 4th tensor\n",
      "cut=16\n",
      "Last SV = 0.06882410114321412\n",
      "virtual_dim=16\n",
      "Factorizing 5th tensor\n",
      "cut=32\n",
      "Last SV = 0.00429207758695352\n",
      "virtual_dim=32\n",
      "Factorizing 6th tensor\n",
      "cut=16\n",
      "Last SV = 0.06623203191747376\n",
      "virtual_dim=16\n",
      "Factorizing 7th tensor\n",
      "cut=8\n",
      "Last SV = 0.14327041528557605\n",
      "virtual_dim=8\n",
      "Factorizing 8th tensor\n",
      "cut=4\n",
      "Last SV = 0.23992763723592606\n",
      "virtual_dim=4\n",
      "Factorizing 9th tensor\n",
      "cut=2\n",
      "Last SV = 0.3539823998818235\n",
      "virtual_dim=2\n",
      "0 - (2, 2)\n",
      "1 - (2, 2, 4)\n",
      "2 - (4, 2, 8)\n",
      "3 - (8, 2, 16)\n",
      "4 - (16, 2, 32)\n",
      "5 - (32, 2, 16)\n",
      "6 - (16, 2, 8)\n",
      "7 - (8, 2, 4)\n",
      "8 - (4, 2, 2)\n",
      "9 - (2, 2)\n",
      "1 - (2, 2) * (2, 2, 4)\n",
      "2 - (4, 4) * (4, 2, 8)\n",
      "3 - (8, 8) * (8, 2, 16)\n",
      "4 - (16, 16) * (16, 2, 32)\n",
      "5 - (32, 32) * (32, 2, 16)\n",
      "6 - (64, 16) * (16, 2, 8)\n",
      "7 - (128, 8) * (8, 2, 4)\n",
      "8 - (256, 4) * (4, 2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = (np.random.rand(1024) + 1j*np.random.rand(1024) )\n",
    "w = w/np.linalg.norm(w)\n",
    "\n",
    "mpsten = tensor_to_mps_svd_trunc(w)\n",
    "\n",
    "len(mpsten)\n",
    "\n",
    "for ii,ten in enumerate(mpsten):\n",
    "    print(f\"{ii} - {np.shape(ten)}\")\n",
    "\n",
    "w_reconstructed = wten_from_mps(mpsten)\n",
    "\n",
    "np.allclose(w, w_reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a random state, chances are that - unless we truncate very aggressively - we don't gain much. \n",
    "\n",
    "But let's try with a different state with more structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wstate(L):\n",
    "    up = np.array([[1],[0]])\n",
    "    do = np.array([[0],[1]])\n",
    "\n",
    "    all_wnn = []\n",
    "\n",
    "    wnn = up\n",
    "    for jj in range(1,L):\n",
    "        wnn = np.kron(wnn, do)\n",
    "\n",
    "    print(np.shape(wnn))\n",
    "\n",
    "    all_wnn.append(wnn)\n",
    "\n",
    "    for nn in range(1,L):\n",
    "        wnn = do\n",
    "        for jj in range(1,L):\n",
    "            if jj == nn:\n",
    "                wnn = np.kron(wnn, do)\n",
    "            else:\n",
    "                wnn = np.kron(wnn, up)\n",
    "        \n",
    "        all_wnn.append(wnn)\n",
    "\n",
    "    wstate = sum(all_wnn)/np.sqrt(L)\n",
    "    return wstate.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First factorize it with QR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1024, 1)\n",
      "1.0\n",
      "nspins=10\n",
      "virtual_dim=2\n",
      "Factorizing 2th tensor\n",
      "virtual_dim=4\n",
      "Factorizing 3th tensor\n",
      "virtual_dim=8\n",
      "Factorizing 4th tensor\n",
      "virtual_dim=16\n",
      "Factorizing 5th tensor\n",
      "virtual_dim=32\n",
      "Factorizing 6th tensor\n",
      "virtual_dim=16\n",
      "Factorizing 7th tensor\n",
      "virtual_dim=8\n",
      "Factorizing 8th tensor\n",
      "virtual_dim=4\n",
      "Factorizing 9th tensor\n",
      "virtual_dim=2\n",
      "0 - (2, 2)\n",
      "1 - (2, 2, 4)\n",
      "2 - (4, 2, 8)\n",
      "3 - (8, 2, 16)\n",
      "4 - (16, 2, 32)\n",
      "5 - (32, 2, 16)\n",
      "6 - (16, 2, 8)\n",
      "7 - (8, 2, 4)\n",
      "8 - (4, 2, 2)\n",
      "9 - (2, 2)\n",
      "1 - (2, 2) * (2, 2, 4)\n",
      "2 - (4, 4) * (4, 2, 8)\n",
      "3 - (8, 8) * (8, 2, 16)\n",
      "4 - (16, 16) * (16, 2, 32)\n",
      "5 - (32, 32) * (32, 2, 16)\n",
      "6 - (64, 16) * (16, 2, 8)\n",
      "7 - (128, 8) * (8, 2, 4)\n",
      "8 - (256, 4) * (4, 2, 2)\n",
      "0.9999999999999997\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wstate = build_wstate(10)\n",
    "print(f\"{np.linalg.norm(wstate)}\")\n",
    "\n",
    "mpsten = tensor_to_mps_qr(wstate)\n",
    "\n",
    "len(mpsten)\n",
    "\n",
    "for ii,ten in enumerate(mpsten):\n",
    "    print(f\"{ii} - {np.shape(ten)}\")\n",
    "\n",
    "w_reconstructed = wten_from_mps(mpsten)\n",
    "print(f\"{np.linalg.norm(w_reconstructed)}\")\n",
    "\n",
    "np.allclose(wstate,w_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32768, 1)\n",
      "0.9999999999999999\n",
      "nspins=15\n",
      "cut=2\n",
      "Last SV = 0.25819888974716115\n",
      "virtual_dim=2\n",
      "Factorizing 2th tensor\n",
      "cut=3\n",
      "Last SV = 0.0\n",
      "virtual_dim=3\n",
      "Factorizing 3th tensor\n",
      "cut=3\n",
      "Last SV = 0.0\n",
      "virtual_dim=3\n",
      "Factorizing 4th tensor\n",
      "cut=3\n",
      "Last SV = 0.0\n",
      "virtual_dim=3\n",
      "Factorizing 5th tensor\n",
      "cut=3\n",
      "Last SV = 6.092980223482375e-33\n",
      "virtual_dim=3\n",
      "Factorizing 6th tensor\n",
      "cut=3\n",
      "Last SV = 1.8025660961824373e-34\n",
      "virtual_dim=3\n",
      "Factorizing 7th tensor\n",
      "cut=3\n",
      "Last SV = 0.0\n",
      "virtual_dim=3\n",
      "Factorizing 8th tensor\n",
      "cut=3\n",
      "Last SV = 8.11640980363574e-49\n",
      "virtual_dim=3\n",
      "Factorizing 9th tensor\n",
      "cut=3\n",
      "Last SV = 2.342480633869552e-37\n",
      "virtual_dim=3\n",
      "Factorizing 10th tensor\n",
      "cut=3\n",
      "Last SV = 2.0680869533763253e-35\n",
      "virtual_dim=3\n",
      "Factorizing 11th tensor\n",
      "cut=3\n",
      "Last SV = 8.48785042906965e-30\n",
      "virtual_dim=3\n",
      "Factorizing 12th tensor\n",
      "cut=3\n",
      "Last SV = 0.0\n",
      "virtual_dim=3\n",
      "Factorizing 13th tensor\n",
      "cut=3\n",
      "Last SV = 1.5081580146586079e-16\n",
      "virtual_dim=3\n",
      "Factorizing 14th tensor\n",
      "cut=2\n",
      "Last SV = 0.3651483716701108\n",
      "virtual_dim=2\n",
      "0 - (2, 2)\n",
      "1 - (2, 2, 3)\n",
      "2 - (3, 2, 3)\n",
      "3 - (3, 2, 3)\n",
      "4 - (3, 2, 3)\n",
      "5 - (3, 2, 3)\n",
      "6 - (3, 2, 3)\n",
      "7 - (3, 2, 3)\n",
      "8 - (3, 2, 3)\n",
      "9 - (3, 2, 3)\n",
      "10 - (3, 2, 3)\n",
      "11 - (3, 2, 3)\n",
      "12 - (3, 2, 3)\n",
      "13 - (3, 2, 2)\n",
      "14 - (2, 2)\n",
      "1 - (2, 2) * (2, 2, 3)\n",
      "2 - (4, 3) * (3, 2, 3)\n",
      "3 - (8, 3) * (3, 2, 3)\n",
      "4 - (16, 3) * (3, 2, 3)\n",
      "5 - (32, 3) * (3, 2, 3)\n",
      "6 - (64, 3) * (3, 2, 3)\n",
      "7 - (128, 3) * (3, 2, 3)\n",
      "8 - (256, 3) * (3, 2, 3)\n",
      "9 - (512, 3) * (3, 2, 3)\n",
      "10 - (1024, 3) * (3, 2, 3)\n",
      "11 - (2048, 3) * (3, 2, 3)\n",
      "12 - (4096, 3) * (3, 2, 3)\n",
      "13 - (8192, 3) * (3, 2, 2)\n",
      "0.9999999999999989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wstate = build_wstate(15)\n",
    "print(f\"{np.linalg.norm(wstate)}\")\n",
    "\n",
    "mpsten = tensor_to_mps_svd_trunc(wstate)\n",
    "\n",
    "len(mpsten)\n",
    "\n",
    "for ii,ten in enumerate(mpsten):\n",
    "    print(f\"{ii} - {np.shape(ten)}\")\n",
    "\n",
    "w_reconstructed = wten_from_mps(mpsten)\n",
    "print(f\"{np.linalg.norm(w_reconstructed)}\")\n",
    "\n",
    "np.allclose(wstate,w_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 - (2, 2)\n",
      "1 - (2, 2, 3)\n",
      "2 - (3, 2, 3)\n",
      "3 - (3, 2, 3)\n",
      "4 - (3, 2, 3)\n",
      "5 - (3, 2, 3)\n",
      "6 - (3, 2, 3)\n",
      "7 - (3, 2, 3)\n",
      "8 - (3, 2, 3)\n",
      "9 - (3, 2, 3)\n",
      "10 - (3, 2, 3)\n",
      "11 - (3, 2, 3)\n",
      "12 - (3, 2, 3)\n",
      "13 - (3, 2, 2)\n",
      "14 - (2, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ii,ten in enumerate(mpsten):\n",
    "    print(f\"{ii} - {np.shape(ten)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
